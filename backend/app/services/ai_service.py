"""
AI Interview Engine â€” Optimized Performance Architecture
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Optimizations:
  â€¢ Model warm-loading at startup (not per-request)
  â€¢ Google Gemini API (gemini-2.5-flash) for fast LLM inference
  â€¢ Two-phase evaluation: instant score (<2s) + background deep analysis
  â€¢ Parallel evaluation with asyncio.gather()
  â€¢ Reduced LLM calls: local scoring for similarity/keywords/communication
  â€¢ Active-time-only timer (pauses during AI processing)
  â€¢ Pre-generation of questions during answer evaluation
"""

import asyncio
import json
import re
from typing import List, Dict, Any, Optional
from datetime import datetime

import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

import google.genai as genai
from google.genai import types as genai_types
from app.core.config import settings

# Import services for report enrichment and integrated AI subsystems
from app.services.explainability_service import explainability_service
from app.services.development_roadmap_service import development_roadmap_service
from app.services.question_generation_service import question_generation_service
from app.services.rl_adaptation_service import rl_adaptation_service


# â”€â”€ Master system prompt injected into every LLM call â”€â”€â”€â”€â”€â”€

MASTER_SYSTEM_PROMPT = """You are an advanced AI Interview Engine designed to simulate a real-world corporate interview.
You must conduct the interview exactly like a senior interviewer at a top company (Google, Microsoft, Amazon level).

CORE RULES:
1. NEVER repeat a question or ask a semantically similar variation of a previously asked question.
2. The interview is TIME-BASED â€” keep generating questions until the allocated time expires.
3. All questions MUST be derived from the Job Description, required skills, tools, and responsibilities.
4. There are TWO rounds: Technical (Round 1) then HR (Round 2).
   - Technical: core skills, problem-solving, scenario-based, tool-specific, system-design questions.
   - HR: behavioral (STAR method), cultural fit, conflict resolution, leadership, career goals, situational judgment.
5. Adapt difficulty based on the candidate's last answer score:
   - Strong (>80%): increase difficulty significantly, ask deeper follow-up, probe edge cases.
   - Moderate (50-80%): ask clarification, probe practical understanding, give a scenario.
   - Weak (<50%): simplify slightly, ask a supportive fallback, or move to an easier related topic.
6. Follow-up questions MUST be context-aware and directly reference the candidate's previous answer.
7. Always generate a comprehensive ideal reference answer (at least 3-4 sentences) and 5-7 evaluation keywords.
8. Always return valid JSON â€” no markdown, no extra text.

QUESTION VARIETY (mix these types across the interview):
- Conceptual: "Explain how X works and why it matters"
- Scenario-based: "Given situation X, how would you approach..."
- Problem-solving: "Design a solution for..."
- Experience-based: "Tell me about a time when..."
- Trade-off analysis: "Compare X vs Y, when would you choose each?"
- Debugging: "This code/system has issue X, how would you diagnose it?"
- System design: "How would you architect a system that..."

IDEAL ANSWER QUALITY:
- The ideal_answer must be a detailed, expert-level response (not generic)
- Include specific technologies, patterns, metrics, or frameworks where applicable
- For HR questions, include STAR method structure in the ideal answer
"""


class AIService:
    """High-performance AI interview engine with warm-loaded models and parallel evaluation."""

    # Maximum cached questions / session counts before cleanup
    _MAX_CACHE_SIZE = 200
    _MAX_SESSION_COUNTS = 500

    def __init__(self):
        self._warmed_up = False
        self._warmup_lock = asyncio.Lock()
        # Cache for pre-generated questions
        self._question_cache: Dict[str, Dict] = {}
        # Track question counts per session for the smart router
        self._session_question_counts: Dict[str, int] = {}

    def cleanup_session(self, session_id: str):
        """Remove session-scoped data to prevent memory leaks."""
        self._session_question_counts.pop(session_id, None)
        # Remove any stale cached questions for this session
        keys_to_remove = [k for k in self._question_cache if session_id in k]
        for k in keys_to_remove:
            self._question_cache.pop(k, None)
        # Enforce global caps
        if len(self._question_cache) > self._MAX_CACHE_SIZE:
            # Evict oldest entries (FIFO)
            excess = len(self._question_cache) - self._MAX_CACHE_SIZE
            for k in list(self._question_cache.keys())[:excess]:
                del self._question_cache[k]

    # â”€â”€ Warm-up: Load models once at startup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def warm_up(self):
        """Lightweight startup â€” initialize shared model registry."""
        if self._warmed_up:
            return
        async with self._warmup_lock:
            if self._warmed_up:
                return
            print("ðŸ”„ Initializing AI service...")

            # Use shared model registry (single instance for all services)
            from app.services.model_registry import model_registry
            model_registry.warm_up()

            if model_registry.gemini_client:
                print(f"  âœ… Gemini configured (model: {settings.GEMINI_MODEL})")
            else:
                print("  âš ï¸ GEMINI_API_KEY not set â€” LLM calls will return empty results")

            self._warmed_up = True
            print("âœ… AI Engine ready â€” models will load on first use")

    async def shutdown(self):
        """Cleanup on app shutdown."""
        pass  # Gemini SDK doesn't require explicit cleanup

    @property
    def embedding_model(self) -> SentenceTransformer:
        from app.services.model_registry import model_registry
        return model_registry.embedding_model

    # â”€â”€ Gemini helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def _gemini_generate(self, prompt: str, system: str = "", fast: bool = False) -> str:
        """Call Google Gemini API with automatic model fallback on quota errors.
        fast=True uses lower token limit."""
        from app.services.model_registry import model_registry
        full_system = MASTER_SYSTEM_PROMPT + "\n\n" + system
        return await model_registry.gemini_generate(prompt, full_system, fast=fast)

    def _parse_json_from_response(self, text: str) -> dict:
        """Extract JSON from LLM response text."""
        json_match = re.search(r"\{[\s\S]*\}", text)
        if json_match:
            try:
                return json.loads(json_match.group())
            except json.JSONDecodeError:
                pass
        return {}

    # â”€â”€ JD Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def analyze_job_description(self, job_description: str, job_title: str) -> Dict[str, Any]:
        """Extract skills, responsibilities, tools, and soft-skill expectations from a JD."""
        prompt = f"""Analyze this Job Description and extract structured information.

Job Title: {job_title}
Job Description:
{job_description}

Return ONLY a JSON object:
{{
  "required_skills": ["skill1", "skill2"],
  "key_responsibilities": ["resp1", "resp2"],
  "tools_and_frameworks": ["tool1", "tool2"],
  "soft_skills": ["soft1", "soft2"],
  "experience_expectations": "summary of expected experience",
  "technical_topics": ["topic1", "topic2"],
  "hr_topics": ["topic1", "topic2"]
}}"""

        response = await self._gemini_generate(prompt, "You are a JD analysis expert. Return valid JSON only.")
        parsed = self._parse_json_from_response(response)
        if not parsed:
            parsed = {
                "required_skills": [job_title, "problem-solving", "communication"],
                "key_responsibilities": ["Perform role duties", "Collaborate with team"],
                "tools_and_frameworks": [],
                "soft_skills": ["teamwork", "communication", "leadership"],
                "experience_expectations": "Relevant industry experience",
                "technical_topics": [job_title],
                "hr_topics": ["motivation", "teamwork", "conflict resolution"],
            }
        return parsed

    # â”€â”€ Question Generation (with pre-generation cache) â”€â”€

    async def generate_question(
        self,
        job_role: str,
        difficulty: str,
        previous_questions: List[str],
        round_type: str = "Technical",
        job_description: str = "",
        experience_level: str = "",
        previous_answers: List[str] = None,
        last_score: float = None,
        jd_analysis: Dict[str, Any] = None,
        is_coding_question: bool = False,
        session_id: str = None,
    ) -> Dict[str, Any]:
        """Generate an adaptive interview question using specialized generators
        with RL-based difficulty calibration and redundancy checking."""

        # â”€â”€ RL-based difficulty adaptation â”€â”€
        calibrated_difficulty = difficulty
        try:
            if session_id:
                # Create RL session if first question
                q_num = self._session_question_counts.get(session_id, 0)
                if q_num == 0:
                    rl_adaptation_service.create_session(session_id, max_questions=15)
                self._session_question_counts[session_id] = q_num + 1

                # Get RL recommendation
                perf = (last_score / 100.0) if last_score is not None else 0.5
                action = rl_adaptation_service.get_next_action(
                    session_id,
                    confidence=perf,
                    performance=perf,
                    stress=max(0, 1 - perf),
                )
                calibrated_difficulty = action.get("recommended_difficulty", difficulty)

                # Also record last score for RL learning
                if last_score is not None and q_num > 0:
                    rl_adaptation_service.record_response(session_id, last_score / 100.0)
        except Exception as e:
            print(f"[RL adaptation] Falling back to heuristic difficulty: {e}")
            calibrated_difficulty = difficulty

        # Also use question_generation_service's difficulty calibration as a cross-check
        if last_score is not None:
            recent_scores = [last_score]
            if previous_answers:
                recent_scores = [last_score]  # Could track more history
            cal_diff = question_generation_service.calibrate_difficulty(
                calibrated_difficulty, recent_scores
            )
            calibrated_difficulty = cal_diff

        # â”€â”€ Route to specialized question generator â”€â”€
        try:
            q_num = self._session_question_counts.get(session_id or "", 1)
            total_planned = 15

            question_data = await question_generation_service.generate_question_smart(
                job_role=job_role,
                difficulty=calibrated_difficulty,
                previous_questions=previous_questions,
                round_type=round_type,
                question_number=q_num,
                total_planned=total_planned,
                jd_analysis=jd_analysis,
                last_score=last_score,
                last_answer=previous_answers[-1] if previous_answers else None,
            )

            # Redundancy check using sentence embeddings
            if question_data and question_data.get("question"):
                is_redundant = question_generation_service.check_question_redundancy(
                    question_data["question"], previous_questions, threshold=0.75
                )
                if is_redundant:
                    print("[QuestionGen] Redundancy detected, falling back to monolithic generator")
                    question_data = None  # Fall through to the fallback

            if question_data and question_data.get("question"):
                # Evaluate quality
                quality = question_generation_service.evaluate_question_quality(question_data)
                if quality.get("overall_quality", 100) < 40:
                    print(f"[QuestionGen] Low quality ({quality.get('overall_quality')}), falling back")
                    question_data = None  # Fall through

        except Exception as e:
            print(f"[QuestionGen] Smart router failed, using fallback: {e}")
            question_data = None

        # â”€â”€ Fallback: monolithic Gemini generator (original logic) â”€â”€
        if not question_data or "question" not in question_data:
            question_data = await self._generate_question_fallback(
                job_role, calibrated_difficulty, previous_questions,
                round_type, job_description, experience_level,
                previous_answers, last_score, jd_analysis, is_coding_question,
            )

        question_data.setdefault("round", round_type)
        question_data.setdefault("evaluation_keywords", question_data.get("keywords", ["experience", "skills"]))
        question_data.setdefault("difficulty_level", calibrated_difficulty)
        question_data.setdefault("is_coding", False)
        question_data.setdefault("followup_trigger_conditions", {})
        question_data["keywords"] = question_data["evaluation_keywords"]

        return question_data

    async def _generate_question_fallback(
        self,
        job_role: str,
        difficulty: str,
        previous_questions: List[str],
        round_type: str = "Technical",
        job_description: str = "",
        experience_level: str = "",
        previous_answers: List[str] = None,
        last_score: float = None,
        jd_analysis: Dict[str, Any] = None,
        is_coding_question: bool = False,
    ) -> Dict[str, Any]:
        """Fallback monolithic question generator using direct Gemini call."""

        prev_q_text = "\n".join(f"- {q}" for q in previous_questions[-30:]) if previous_questions else "None"
        prev_a_text = ""
        if previous_answers and len(previous_answers) > 0:
            last_answer = previous_answers[-1] if previous_answers else ""
            prev_a_text = f"\nCandidate's last answer: {last_answer}"

        followup_instruction = ""
        if last_score is not None:
            if last_score >= 80:
                followup_instruction = "The candidate scored well. INCREASE difficulty. Ask a deeper technical follow-up related to their last answer."
            elif last_score >= 50:
                followup_instruction = "The candidate gave a moderate answer. Ask a clarification question or probe their practical understanding."
            else:
                followup_instruction = "The candidate struggled. Ask a simpler, supportive question on a related topic or move to an easier area."

        jd_context = ""
        if job_description:
            jd_context = f"\nFull Job Description:\n{job_description}\n"
        if jd_analysis:
            jd_context += f"\nExtracted Skills: {json.dumps(jd_analysis.get('required_skills', []))}"
            jd_context += f"\nKey Responsibilities: {json.dumps(jd_analysis.get('key_responsibilities', []))}"
            jd_context += f"\nTools & Frameworks: {json.dumps(jd_analysis.get('tools_and_frameworks', []))}"
            if round_type == "HR":
                jd_context += f"\nSoft Skills to Evaluate: {json.dumps(jd_analysis.get('soft_skills', []))}"
                jd_context += f"\nHR Topics: {json.dumps(jd_analysis.get('hr_topics', []))}"
            else:
                jd_context += f"\nTechnical Topics: {json.dumps(jd_analysis.get('technical_topics', []))}"

        coding_instruction = ""
        if is_coding_question:
            coding_instruction = """
This must be a CODING question. Ask the candidate to write code to solve a specific problem.
Include in the question: the problem statement, expected input/output, and any constraints.
The ideal_answer should contain a working code solution.
Set "is_coding": true in the response."""

        # Add randomization seed for variety across sessions
        import random
        variety_seed = random.randint(1, 10000)
        topic_angles = [
            "a practical scenario", "a conceptual deep-dive", "a real-world problem",
            "a comparison or trade-off analysis", "a design challenge",
            "an optimization problem", "a debugging scenario", "a best-practices discussion",
            "an architecture decision", "a recent technology trend",
        ]
        chosen_angle = random.choice(topic_angles)

        prompt = f"""Generate a {round_type} interview question for a {job_role} position.
Experience Level: {experience_level or 'Not specified'}
Difficulty: {difficulty}
Round: {round_type}
{jd_context}

Previously asked questions (DO NOT repeat these or ask semantically similar questions â€” pick a DIFFERENT topic/angle each time):
{prev_q_text}
{prev_a_text}

{followup_instruction}
{coding_instruction}

CRITICAL RULES:
1. The question MUST be SHORT and CONCISE â€” ideally 1-2 sentences (max 30 words).
2. Do NOT add long preambles, context paragraphs, or multi-part questions.
3. Ask ONE clear thing. Examples of GOOD questions:
   - "What is the difference between an abstract class and an interface?"
   - "How would you optimize a slow database query?"
   - "Tell me about a time you resolved a team conflict."
4. BAD questions are overly long, multi-part, or contain unnecessary context.
5. The ideal_answer should be a concise model answer (3-5 sentences).
6. Create a UNIQUE question DIFFERENT from all previously asked questions.
7. Approach this from the angle of: {chosen_angle}.

Variety seed: {variety_seed}

Return ONLY a JSON object in this exact format:
{{
  "round": "{round_type}",
  "question": "Your SHORT interview question here (1-2 sentences max)",
  "ideal_answer": "Concise ideal answer (3-5 sentences)",
  "evaluation_keywords": ["keyword1", "keyword2", "keyword3", "keyword4", "keyword5"],
  "difficulty_level": "{difficulty}",
  "is_coding": false,
  "followup_trigger_conditions": {{
    "strong_answer": "Harder follow-up question (1 sentence)",
    "moderate_answer": "Clarification follow-up (1 sentence)",
    "weak_answer": "Simpler fallback question (1 sentence)"
  }}
}}"""

        system = f"You are an expert {round_type} interviewer. Generate SHORT, CONCISE, and relevant questions (1-2 sentences max). Never write long or multi-part questions. Always return valid JSON."

        response = await self._gemini_generate(prompt, system)
        parsed = self._parse_json_from_response(response)

        if not parsed or "question" not in parsed:
            if round_type == "HR":
                fallback_questions = [
                    "Tell me about a time you handled a conflict in your team.",
                    "What motivates you in your career?",
                    "Describe a situation where you showed leadership.",
                    "Where do you see yourself in five years?",
                    "How do you handle tight deadlines?",
                    "What is your biggest professional achievement?",
                    "Why are you interested in this role?",
                    "How do you prioritize when everything is urgent?",
                ]
            else:
                fallback_questions = [
                    f"What are the key principles of {job_role}?",
                    f"Describe a tough technical problem you solved recently.",
                    f"What tools and frameworks do you prefer as a {job_role} and why?",
                    f"How would you design a scalable system for a typical {job_role} task?",
                    f"What is your approach to debugging production issues?",
                    f"Explain a complex {job_role} concept in simple terms.",
                    f"What are common performance bottlenecks in {job_role} work?",
                    f"How do you ensure code quality in your projects?",
                ]

            chosen = fallback_questions[0]
            for fq in fallback_questions:
                if fq not in previous_questions:
                    chosen = fq
                    break

            parsed = {
                "round": round_type,
                "question": chosen,
                "ideal_answer": "A strong answer should cover relevant experience, specific examples, and demonstrate domain knowledge.",
                "evaluation_keywords": ["experience", "skills", "knowledge", "examples", "approach"],
                "difficulty_level": difficulty,
                "is_coding": False,
                "followup_trigger_conditions": {},
            }

        return parsed

    # â”€â”€ Pre-generate next question (fire-and-forget) â”€â”€

    async def pre_generate_question(self, cache_key: str, **kwargs):
        """Pre-generate the next question in the background while evaluation runs."""
        try:
            q = await self.generate_question(**kwargs)
            self._question_cache[cache_key] = q
        except Exception as e:
            print(f"Pre-generation failed: {e}")

    def get_cached_question(self, cache_key: str) -> Optional[Dict]:
        """Get a pre-generated question from the cache."""
        return self._question_cache.pop(cache_key, None)

    # â”€â”€ TWO-PHASE ANSWER EVALUATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #
    # Phase 1 (Instant, < 2s): Semantic similarity + keyword match + communication heuristics
    # Phase 2 (Background):    LLM depth analysis + AI feedback
    #

    def evaluate_answer_instant(
        self,
        question: str,
        ideal_answer: str,
        candidate_answer: str,
        keywords: List[str],
        round_type: str = "Technical",
    ) -> Dict[str, Any]:
        """Phase 1: Instant scoring using local models only (no LLM calls).
        Returns a score within ~1-2 seconds.
        """
        if not candidate_answer.strip():
            return {
                "content_score": 0, "keyword_score": 0, "depth_score": 0,
                "communication_score": 0, "confidence_score": 0, "overall_score": 0,
                "similarity_score": 0, "keyword_coverage": 0,
                "keywords_matched": [], "keywords_missed": keywords,
                "feedback": "No answer provided.",
                "answer_strength": "weak",
                "phase": "instant",
            }

        # 1. Semantic similarity (SentenceTransformer â€” local, fast)
        embeddings = self.embedding_model.encode([ideal_answer, candidate_answer])
        sim_score = float(cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]) * 100

        # 2. Keyword coverage (pure string match â€” instant)
        answer_lower = candidate_answer.lower()
        matched = [k for k in keywords if k.lower() in answer_lower]
        missed = [k for k in keywords if k.lower() not in answer_lower]
        keyword_pct = (len(matched) / max(len(keywords), 1)) * 100

        # 3. Communication score (heuristic â€” instant)
        word_count = len(candidate_answer.split())
        sentences = [s.strip() for s in candidate_answer.split(".") if s.strip()]
        # Base score from response length
        if word_count < 10:
            comm_score = 15
        elif word_count < 20:
            comm_score = 35
        elif word_count < 50:
            comm_score = 55
        elif word_count < 100:
            comm_score = 70
        elif word_count < 200:
            comm_score = 82
        else:
            comm_score = 88
        # Bonus for structured multi-sentence answers
        if len(sentences) >= 3:
            comm_score = min(100, comm_score + 8)
        if len(sentences) >= 5:
            comm_score = min(100, comm_score + 5)
        # Bonus for transition words indicating structured thinking
        structure_markers = ['firstly', 'secondly', 'however', 'moreover', 'for example',
                            'in addition', 'furthermore', 'therefore', 'in conclusion',
                            'on the other hand', 'specifically', 'for instance']
        marker_count = sum(1 for m in structure_markers if m in candidate_answer.lower())
        comm_score = min(100, comm_score + marker_count * 3)

        # 4. Depth estimate (heuristic based on similarity + length + keywords)
        depth_score = min(100, sim_score * 0.5 + keyword_pct * 0.3 + min(word_count, 100) * 0.2)

        # 5. Content accuracy
        content_score = (sim_score * 0.6) + (keyword_pct * 0.4)

        # 6. Confidence placeholder
        confidence_score = 50.0

        # 7. Overall score with master weights
        overall = (
            content_score * 0.40
            + keyword_pct * 0.20
            + depth_score * 0.15
            + comm_score * 0.15
            + confidence_score * 0.10
        )

        if overall >= 80:
            answer_strength = "strong"
        elif overall >= 50:
            answer_strength = "moderate"
        else:
            answer_strength = "weak"

        # Detailed heuristic feedback (no LLM)
        feedback_parts = []
        if sim_score >= 70:
            feedback_parts.append("Your answer aligns well with the expected response.")
        elif sim_score >= 40:
            feedback_parts.append("Your answer partially covers the expected content.")
        else:
            feedback_parts.append("Your answer doesn't closely match what was expected.")

        if keyword_pct >= 70:
            feedback_parts.append("Good use of relevant technical terminology.")
        elif missed:
            feedback_parts.append(f"Consider mentioning: {', '.join(missed[:3])}.")

        if word_count < 30:
            feedback_parts.append("Try to elaborate more â€” provide specific examples and details.")
        elif len(sentences) < 3:
            feedback_parts.append("Structure your answer into multiple points for clarity.")

        if overall >= 75:
            feedback_parts.append("Strong response overall!")
        elif overall < 40:
            feedback_parts.append("Review the core concepts and practice with concrete examples.")

        feedback = " ".join(feedback_parts)

        return {
            "content_score": round(content_score, 1),
            "keyword_score": round(keyword_pct, 1),
            "depth_score": round(depth_score, 1),
            "communication_score": round(comm_score, 1),
            "confidence_score": round(confidence_score, 1),
            "overall_score": round(overall, 1),
            "similarity_score": round(sim_score, 1),
            "keyword_coverage": round(keyword_pct, 1),
            "keywords_matched": matched,
            "keywords_missed": missed,
            "feedback": feedback,
            "answer_strength": answer_strength,
            "phase": "instant",
        }

    async def evaluate_answer_deep(
        self,
        question: str,
        ideal_answer: str,
        candidate_answer: str,
        keywords: List[str],
        instant_result: Dict[str, Any],
        round_type: str = "Technical",
    ) -> Dict[str, Any]:
        """Phase 2: Deep analysis using LLM (runs in background).
        Enhances the instant result with LLM depth and feedback.
        """
        try:
            # Run depth evaluation and feedback generation in parallel
            depth_task = self._evaluate_depth(question, candidate_answer, instant_result["similarity_score"])
            feedback_task = self._get_ai_feedback(question, candidate_answer, instant_result["overall_score"], round_type)

            depth_score, feedback = await asyncio.gather(depth_task, feedback_task)

            # Recalculate overall with real depth score
            content_score = instant_result["content_score"]
            keyword_pct = instant_result["keyword_score"]
            comm_score = instant_result["communication_score"]
            confidence_score = instant_result["confidence_score"]

            overall = (
                content_score * 0.40
                + keyword_pct * 0.20
                + depth_score * 0.15
                + comm_score * 0.15
                + confidence_score * 0.10
            )

            if overall >= 80:
                answer_strength = "strong"
            elif overall >= 50:
                answer_strength = "moderate"
            else:
                answer_strength = "weak"

            return {
                **instant_result,
                "depth_score": round(depth_score, 1),
                "overall_score": round(overall, 1),
                "feedback": feedback if feedback else instant_result["feedback"],
                "answer_strength": answer_strength,
                "phase": "deep",
            }
        except Exception as e:
            print(f"Deep evaluation error: {e}")
            return {**instant_result, "phase": "deep_failed"}

    async def evaluate_answer(
        self,
        question: str,
        ideal_answer: str,
        candidate_answer: str,
        keywords: List[str],
        round_type: str = "Technical",
        is_coding: bool = False,
    ) -> Dict[str, Any]:
        """Full evaluation: runs instant first, then deep in parallel.
        Returns the best available result.
        """
        # Phase 1: Instant (< 2s)
        instant = self.evaluate_answer_instant(
            question, ideal_answer, candidate_answer, keywords, round_type
        )

        # Phase 2: Deep (parallel LLM calls)
        try:
            deep = await asyncio.wait_for(
                self.evaluate_answer_deep(
                    question, ideal_answer, candidate_answer, keywords, instant, round_type
                ),
                timeout=15.0,  # Don't wait more than 15s for deep analysis
            )
            return deep
        except asyncio.TimeoutError:
            print("âš ï¸ Deep evaluation timed out, using instant scores")
            return instant

    async def _evaluate_depth(self, question: str, answer: str, sim_score: float) -> float:
        """Use LLM to evaluate depth of knowledge in the answer."""
        prompt = f"""Rate the depth of knowledge shown in this interview answer on a scale of 0-100.

Question: {question}
Answer: {answer}

Consider:
- Does the answer go beyond surface level?
- Are specific examples, frameworks, or methodologies mentioned?
- Does it show practical experience?

Return ONLY a JSON object: {{"depth_score": <number>}}"""

        try:
            response = await self._gemini_generate(prompt, "You are an expert evaluator. Return only valid JSON.", fast=True)
            parsed = self._parse_json_from_response(response)
            score = parsed.get("depth_score", sim_score * 0.8)
            return max(0, min(100, float(score)))
        except Exception:
            return sim_score * 0.8

    async def _get_ai_feedback(
        self, question: str, answer: str, score: float, round_type: str = "Technical"
    ) -> str:
        prompt = f"""Evaluate this {round_type} interview answer briefly (2-3 sentences).
Question: {question}
Answer: {answer}
Score: {score}/100

Provide constructive feedback: what was good, what could be improved, and one specific suggestion."""

        system = "You are an expert interviewer providing brief, constructive, actionable feedback."
        try:
            result = await self._gemini_generate(prompt, system, fast=True)
            if result.strip():
                return result.strip()
        except Exception:
            pass

        if score >= 70:
            return "Good answer with relevant details. Consider adding more specific examples to strengthen your response."
        elif score >= 40:
            return "Decent answer but could be more detailed. Include specific examples and demonstrate deeper knowledge."
        else:
            return "Answer needs improvement. Focus on addressing the question directly with relevant examples and key concepts."

    # â”€â”€ Evaluate Code Submission â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def evaluate_code(
        self,
        question: str,
        ideal_answer: str,
        submitted_code: str,
        language: str = "python",
    ) -> Dict[str, Any]:
        """Evaluate a coding question submission."""
        prompt = f"""Evaluate this code submission for an interview coding question.

Question: {question}
Expected Solution: {ideal_answer}
Submitted Code ({language}):
```{language}
{submitted_code}
```

Evaluate on:
1. Correctness (does it solve the problem?) - 0-100
2. Code quality (readability, naming, structure) - 0-100
3. Efficiency (time/space complexity) - 0-100
4. Edge case handling - 0-100

Also generate 2-3 follow-up questions about the code logic.

Return ONLY a JSON object:
{{
  "correctness_score": <number>,
  "quality_score": <number>,
  "efficiency_score": <number>,
  "edge_case_score": <number>,
  "overall_score": <number>,
  "feedback": "Brief constructive feedback",
  "follow_up_questions": ["q1", "q2"]
}}"""

        response = await self._gemini_generate(prompt, "You are an expert code reviewer. Return valid JSON only.")
        parsed = self._parse_json_from_response(response)

        if not parsed or "overall_score" not in parsed:
            embeddings = self.embedding_model.encode([ideal_answer, submitted_code])
            sim = float(cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]) * 100
            parsed = {
                "correctness_score": round(sim, 1),
                "quality_score": 50.0,
                "efficiency_score": 50.0,
                "edge_case_score": 40.0,
                "overall_score": round(sim * 0.8, 1),
                "feedback": "Code submitted. Review the expected solution for comparison.",
                "follow_up_questions": [
                    "Can you explain the time complexity of your solution?",
                    "How would you handle edge cases?",
                ],
            }

        return parsed

    # â”€â”€ Round Transition Logic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def calculate_round_score(self, responses: List[dict]) -> float:
        """Calculate average overall score for a set of responses."""
        if not responses:
            return 0.0
        scores = [r.get("evaluation", {}).get("overall_score", 0) for r in responses]
        return round(sum(scores) / len(scores), 1)

    def should_proceed_to_hr(self, technical_score: float, cutoff: float = 70.0) -> bool:
        """Check if candidate qualifies for HR round."""
        return technical_score >= cutoff

    def determine_next_difficulty(self, last_score: float, current_difficulty: str) -> str:
        """Adapt difficulty based on last answer performance."""
        if last_score >= 80:
            return "hard"
        elif last_score >= 50:
            return "medium"
        else:
            return "easy"

    # â”€â”€ ACTIVE-TIME TIMER (pauses during AI processing) â”€â”€

    def check_time_status(
        self,
        start_time: datetime,
        duration_minutes: int,
        processing_time_seconds: float = 0,
    ) -> Dict[str, Any]:
        """Check interview time status using ACTIVE TIME only.
        
        Subtracts cumulative AI processing time from elapsed time
        so candidates aren't penalized for slow evaluation.
        """
        now = datetime.utcnow()
        wall_elapsed = (now - start_time).total_seconds() / 60
        # Subtract processing overhead from elapsed time
        active_elapsed = max(0, wall_elapsed - (processing_time_seconds / 60))
        remaining = max(0, duration_minutes - active_elapsed)

        return {
            "elapsed_minutes": round(active_elapsed, 1),
            "remaining_minutes": round(remaining, 1),
            "remaining_seconds": int(remaining * 60),
            "is_expired": remaining <= 0,
            "is_wrap_up": 0 < remaining < 2,
            "progress_pct": min(100, round((active_elapsed / max(duration_minutes, 1)) * 100, 1)),
            "wall_elapsed_minutes": round(wall_elapsed, 1),
        }

    # â”€â”€ Report Generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def generate_report(self, session: dict, user: dict) -> dict:
        """Generate comprehensive two-round interview report."""
        questions = session.get("questions", [])
        responses = session.get("responses", [])

        tech_evaluations = []
        hr_evaluations = []
        all_scores = {
            "content": [], "keyword": [], "depth": [],
            "communication": [], "confidence": [], "overall": [],
        }

        for resp in responses:
            q_doc = next(
                (q for q in questions if q["question_id"] == resp["question_id"]),
                None,
            )
            if not q_doc:
                continue

            ev = resp.get("evaluation", {})
            round_type = q_doc.get("round", "Technical")

            eval_entry = {
                "question": q_doc["question"],
                "answer": resp["answer_text"],
                "ideal_answer": q_doc.get("ideal_answer", ""),
                "round": round_type,
                "difficulty": q_doc.get("difficulty", "medium"),
                "scores": {
                    "content_score": ev.get("content_score", 0),
                    "keyword_score": ev.get("keyword_score", ev.get("keyword_coverage", 0)),
                    "depth_score": ev.get("depth_score", 0),
                    "communication_score": ev.get("communication_score", 0),
                    "confidence_score": ev.get("confidence_score", 50),
                    "overall_score": ev.get("overall_score", 0),
                },
                "feedback": ev.get("feedback", ""),
                "keywords_matched": ev.get("keywords_matched", []),
                "keywords_missed": ev.get("keywords_missed", []),
                "answer_strength": ev.get("answer_strength", "moderate"),
            }

            if round_type == "HR":
                hr_evaluations.append(eval_entry)
            else:
                tech_evaluations.append(eval_entry)

            for key in ["content", "keyword", "depth", "communication", "confidence", "overall"]:
                score_key = f"{key}_score"
                all_scores[key].append(ev.get(score_key, ev.get(key, 0)))

        def safe_avg(lst):
            return round(sum(lst) / max(len(lst), 1), 1)

        tech_scores = [e["scores"]["overall_score"] for e in tech_evaluations]
        hr_scores = [e["scores"]["overall_score"] for e in hr_evaluations]
        tech_avg = safe_avg(tech_scores)
        hr_avg = safe_avg(hr_scores)
        overall_avg = safe_avg(tech_scores + hr_scores)

        overall_scores = {
            "content_score": safe_avg(all_scores["content"]),
            "keyword_score": safe_avg(all_scores["keyword"]),
            "depth_score": safe_avg(all_scores["depth"]),
            "communication_score": safe_avg(all_scores["communication"]),
            "confidence_score": safe_avg(all_scores["confidence"]),
            "overall_score": overall_avg,
        }

        strengths, weaknesses, suggestions = self._analyze_performance(
            overall_scores, tech_evaluations + hr_evaluations
        )

        if tech_avg >= 70 and hr_avg >= 60:
            recommendation = "Selected"
            confidence_analysis = "Strong candidate with good technical and interpersonal skills."
        elif tech_avg >= 70:
            recommendation = "Maybe â€” HR skills need improvement"
            confidence_analysis = "Technically strong but needs improvement in soft skills."
        elif tech_avg >= 50:
            recommendation = "Not Selected â€” Below threshold"
            confidence_analysis = "Candidate shows potential but did not meet the required technical cutoff."
        else:
            recommendation = "Not Selected"
            confidence_analysis = "Candidate needs significant improvement in technical knowledge."

        comm_avg = overall_scores["communication_score"]
        if comm_avg >= 80:
            comm_feedback = "Excellent communication skills. Answers are well-structured and articulate."
        elif comm_avg >= 60:
            comm_feedback = "Good communication. Could improve answer structure and depth."
        elif comm_avg >= 40:
            comm_feedback = "Average communication. Needs to practice structuring responses clearly."
        else:
            comm_feedback = "Communication needs significant improvement. Practice the STAR method for behavioral questions."

        # â”€â”€ Explainability Service: SHAP-based dimension analysis â”€â”€
        try:
            avg_answer_text = " ".join(
                e.get("answer", "")[:200] for e in (tech_evaluations + hr_evaluations)[:5]
            )
            explainability_eval = {
                "content_score": overall_scores["content_score"],
                "similarity_score": overall_scores["content_score"],
                "keyword_coverage": overall_scores["keyword_score"],
                "keyword_score": overall_scores["keyword_score"],
                "depth_score": overall_scores["depth_score"],
                "communication_score": overall_scores["communication_score"],
                "confidence_score": overall_scores["confidence_score"],
                "fluency_score": overall_scores.get("communication_score", 50),
                "eye_contact": overall_scores.get("confidence_score", 50),
                "emotion_stability": max(50, overall_scores.get("confidence_score", 50) - 5),
                "stress_level": max(0, 100 - overall_scores.get("confidence_score", 50)),
                "facial_confidence": overall_scores.get("confidence_score", 50),
                "specificity_score": overall_scores.get("depth_score", 50),
                "answer_text": avg_answer_text,
            }
            explainability_result = explainability_service.explain_score(explainability_eval)
        except Exception as e:
            print(f"[Report] Explainability service error: {e}")
            explainability_result = None

        # â”€â”€ Development Roadmap Service: personalized improvement plan â”€â”€
        try:
            # Build dimension_scores dict matching roadmap service expectations
            dim_scores_for_roadmap = {}
            if explainability_result and "dimension_scores" in explainability_result:
                dim_scores_for_roadmap = explainability_result["dimension_scores"]
            else:
                # Fallback: build from raw scores
                def _grade(s):
                    if s >= 85: return "Excellent"
                    if s >= 70: return "Good"
                    if s >= 55: return "Average"
                    if s >= 40: return "Below Average"
                    return "Needs Improvement"

                dim_scores_for_roadmap = {
                    "Communication": {"score": overall_scores["communication_score"], "grade": _grade(overall_scores["communication_score"])},
                    "Technical Depth": {"score": overall_scores["content_score"], "grade": _grade(overall_scores["content_score"])},
                    "Confidence": {"score": overall_scores["confidence_score"], "grade": _grade(overall_scores["confidence_score"])},
                    "Emotional Regulation": {"score": max(50, overall_scores["confidence_score"] - 5), "grade": _grade(max(50, overall_scores["confidence_score"] - 5))},
                    "Problem Solving": {"score": overall_scores["depth_score"], "grade": _grade(overall_scores["depth_score"])},
                }

            roadmap_eval_summary = {
                "overall_score": overall_avg,
                "dimension_scores": dim_scores_for_roadmap,
                "improvement_suggestions": (
                    explainability_result.get("improvement_suggestions", [])
                    if explainability_result else []
                ),
            }
            job_role = session.get("job_role", "")
            development_roadmap = development_roadmap_service.generate_roadmap(
                roadmap_eval_summary, target_role=job_role, weeks_available=8
            )
        except Exception as e:
            print(f"[Report] Development roadmap service error: {e}")
            development_roadmap = None

        return {
            "session_id": str(session.get("_id", "")),
            "candidate_name": user.get("name", "Candidate"),
            "job_role": session.get("job_role", ""),
            "total_questions": len(responses),
            "technical_questions": len(tech_evaluations),
            "hr_questions": len(hr_evaluations),
            "technical_score": tech_avg,
            "hr_score": hr_avg,
            "overall_score": overall_avg,
            "overall_scores": overall_scores,
            "question_evaluations": tech_evaluations + hr_evaluations,
            "strengths": strengths,
            "weaknesses": weaknesses,
            "improvement_suggestions": suggestions,
            "communication_feedback": comm_feedback,
            "confidence_analysis": confidence_analysis,
            "recommendation": recommendation,
            "round_summary": {
                "technical": {
                    "score": tech_avg,
                    "questions_asked": len(tech_evaluations),
                    "passed": tech_avg >= 70,
                },
                "hr": {
                    "score": hr_avg,
                    "questions_asked": len(hr_evaluations),
                    "passed": hr_avg >= 60,
                },
            },
            "generated_at": datetime.utcnow().isoformat(),
            # â”€â”€ Enriched analysis from integrated services â”€â”€
            "explainability": explainability_result,
            "development_roadmap": development_roadmap,
            # â”€â”€ Proctoring data â”€â”€
            "proctoring": session.get("proctoring", {}),
        }

    def _analyze_performance(self, scores: dict, evaluations: list) -> tuple:
        """Generate dynamic, interview-specific strengths, weaknesses, and suggestions
        based on actual question-level performance data."""
        strengths = []
        weaknesses = []
        suggestions = []

        content = scores.get("content_score", 0)
        comm = scores.get("communication_score", 0)
        depth = scores.get("depth_score", 0)
        keyword = scores.get("keyword_score", 0)
        confidence = scores.get("confidence_score", 0)
        overall = scores.get("overall_score", 0)

        # â”€â”€ Dimension-level analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if content >= 70:
            strengths.append(f"Strong technical knowledge (Content: {content:.0f}%)")
        else:
            weaknesses.append(f"Content relevance needs work (Content: {content:.0f}%)")

        if comm >= 70:
            strengths.append(f"Clear and structured communication (Communication: {comm:.0f}%)")
        else:
            weaknesses.append(f"Communication could be more structured (Communication: {comm:.0f}%)")

        if depth >= 70:
            strengths.append(f"Good depth of understanding (Depth: {depth:.0f}%)")
        else:
            weaknesses.append(f"Answers lack depth and detail (Depth: {depth:.0f}%)")

        if keyword >= 70:
            strengths.append(f"Effective use of domain terminology (Keywords: {keyword:.0f}%)")
        else:
            weaknesses.append(f"Missing key technical terms (Keywords: {keyword:.0f}%)")

        if confidence >= 70:
            strengths.append(f"Confident and composed delivery (Confidence: {confidence:.0f}%)")
        elif confidence < 45:
            weaknesses.append(f"Appeared nervous or uncertain (Confidence: {confidence:.0f}%)")

        # â”€â”€ Question-level analysis: find specific weak topics â”€â”€
        weak_questions = []
        strong_questions = []
        all_missed_keywords = []
        weak_topics = set()
        strong_topics = set()

        for e in evaluations:
            q_score = e.get("scores", {}).get("overall_score", 0)
            q_text = e.get("question", "")
            topic = e.get("topic", "") or e.get("question_subtype", "")
            missed = e.get("keywords_missed", [])
            round_type = e.get("round", "Technical")

            if q_score < 50:
                weak_questions.append({"question": q_text, "score": q_score, "round": round_type})
                if topic:
                    weak_topics.add(topic)
            elif q_score >= 75:
                strong_questions.append({"question": q_text, "score": q_score, "round": round_type})
                if topic:
                    strong_topics.add(topic)

            all_missed_keywords.extend(missed)

        # Report specific struggled questions
        if weak_questions:
            weak_count = len(weak_questions)
            total = len(evaluations)
            weaknesses.append(f"Struggled with {weak_count}/{total} questions (scored below 50%)")

            # Show the weakest questions specifically
            worst = sorted(weak_questions, key=lambda x: x["score"])[:3]
            for w in worst:
                short_q = w["question"][:60] + "..." if len(w["question"]) > 60 else w["question"]
                weaknesses.append(f"  Low score on: \"{short_q}\" ({w['score']:.0f}%)")

        if strong_questions and len(strong_questions) >= 2:
            strengths.append(f"Excelled in {len(strong_questions)}/{len(evaluations)} questions (scored 75%+)")

        # â”€â”€ Dynamic suggestions based on actual gaps â”€â”€
        # Sort dimensions by score to prioritize weakest areas
        dims = [
            ("Content", content, "Study core concepts for the role. Review textbooks, documentation, and practice explaining topics out loud."),
            ("Communication", comm, "Practice the STAR method (Situation, Task, Action, Result). Record yourself answering and review for clarity."),
            ("Depth", depth, "Go deeper in your answers. Include specific examples, metrics, trade-offs, and real-world scenarios."),
            ("Keywords", keyword, "Review job descriptions for your target role. Use relevant technical terms naturally in your answers."),
            ("Confidence", confidence, "Practice mock interviews regularly. Prepare 2-3 strong examples for common question types."),
        ]
        dims_sorted = sorted(dims, key=lambda d: d[1])

        # Suggest improvements for the weakest 2-3 dimensions
        for name, score, suggestion in dims_sorted:
            if score < 70:
                suggestions.append(f"[{name} - {score:.0f}%] {suggestion}")
            if len(suggestions) >= 3 and score >= 50:
                break  # Enough suggestions for moderate performers

        # Keyword-specific suggestions
        if all_missed_keywords:
            # Get top missed keywords (most frequently missed)
            from collections import Counter
            keyword_counts = Counter(all_missed_keywords)
            top_missed = [kw for kw, _ in keyword_counts.most_common(5)]
            suggestions.append(f"Focus on these missed keywords: {', '.join(top_missed)}")

        # Weak topic suggestions
        if weak_topics:
            suggestions.append(f"Revise these weak areas: {', '.join(list(weak_topics)[:4])}")

        # Round-specific advice
        tech_evals = [e for e in evaluations if e.get("round") != "HR"]
        hr_evals = [e for e in evaluations if e.get("round") == "HR"]
        if tech_evals:
            tech_avg = sum(e.get("scores", {}).get("overall_score", 0) for e in tech_evals) / len(tech_evals)
            if tech_avg < 50:
                suggestions.append("Technical round needs significant work. Focus on fundamentals and practice coding problems daily.")
        if hr_evals:
            hr_avg = sum(e.get("scores", {}).get("overall_score", 0) for e in hr_evals) / len(hr_evals)
            if hr_avg < 50:
                suggestions.append("HR round needs improvement. Prepare stories about teamwork, leadership, and conflict resolution.")

        # Ensure we always have at least one suggestion
        if not strengths:
            strengths.append("Shows willingness to practice and improve")
        if not suggestions:
            suggestions.append("Maintain your strong performance by continuing regular practice")

        return strengths, weaknesses, suggestions


# Singleton
ai_service = AIService()
