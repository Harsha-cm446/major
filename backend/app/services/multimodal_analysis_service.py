"""
Multimodal Analysis Engine
────────────────────────────────────────
Component 3: Real-time multimodal candidate analysis
  • Facial Expression Recognition (FER+ / DeepFace)
  • Voice Sentiment Analysis (speech features)
  • Eye Tracking / Gaze Estimation
  • Body Posture Detection
  • Speech Fluency Metrics
  • Attention-based Temporal Fusion

Pipeline:
  Video Frame ──▶ Face Detection ──▶ Emotion Recognition ──▶ ┐
  Audio Chunk  ──▶ Voice Features ──▶ Sentiment Analysis  ──▶ │
  Gaze Data    ──▶ Eye Tracking   ──▶ Attention Score     ──▶ ├──▶ Fusion ──▶ Metrics
  Posture Data ──▶ Body Analysis  ──▶ Engagement Score    ──▶ │
  Transcript   ──▶ Fluency Calc   ──▶ Clarity Score       ──▶ ┘

Feature Alignment Strategy:
  All modalities are resampled to 1Hz (1 reading/second)
  Temporal modeling via sliding window LSTM / Transformer

Fusion Mechanism:
  Attention-based cross-modal fusion with learned weights
"""

import time
import math
import base64
from typing import Dict, Any, List, Optional, Tuple
from collections import deque
from datetime import datetime
from enum import Enum

import numpy as np

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False

try:
    from deepface import DeepFace
    DEEPFACE_AVAILABLE = True
except (ImportError, ValueError, Exception) as e:
    DeepFace = None
    DEEPFACE_AVAILABLE = False
    print(f"\u26a0\ufe0f DeepFace unavailable: {e}")

try:
    from ultralytics import YOLO
    _yolo_model = YOLO("yolov8n.pt")
    YOLO_AVAILABLE = True
except Exception:
    _yolo_model = None
    YOLO_AVAILABLE = False



# ══════════════════════════════════════════════════════════════════════
# Gaze Finite State Machine — production-ready eye contact monitoring
# ══════════════════════════════════════════════════════════════════════

class GazeState(str, Enum):
    """Possible states of the gaze monitoring FSM."""
    ATTENTIVE = "ATTENTIVE"               # Candidate is looking at the camera
    WARNING_ACTIVE = "WARNING_ACTIVE"     # Sustained gaze deviation → show warning
    RECOVERING = "RECOVERING"             # Gaze returning, not yet stable


class GazeStateMachine:
    """
    Finite-state-machine for robust eye-contact monitoring.

    Design principles:
      • No state change on a single frame — uses a rolling window percentage.
      • Separate timers for deviation and recovery — never reused across states.
      • Time thresholds prevent flicker on rapid head movements.
      • Handles frame drops and camera freeze via a staleness timeout.

    States & transitions:
      ATTENTIVE ──(away >70% for ≥2.5 s)──▶ WARNING_ACTIVE
      WARNING_ACTIVE ──(looking >70% for ≥0.5 s)──▶ RECOVERING
      RECOVERING ──(looking >70% for ≥1.5 s)──▶ ATTENTIVE
      RECOVERING ──(away >70% again)──▶ WARNING_ACTIVE

    Parameters:
        window_size:         Number of frames in the rolling window (default 45)
        away_pct_threshold:  Fraction of window frames that must be "away" (0.70)
        look_pct_threshold:  Fraction that must be "looking" to start recovery (0.70)
        deviation_hold_sec:  Seconds of sustained "away" before WARNING_ACTIVE (2.5)
        recovery_entry_sec:  Seconds of sustained "looking" to enter RECOVERING (0.5)
        recovery_full_sec:   Seconds of sustained "looking" to reach ATTENTIVE (1.5)
        gaze_threshold:      Score below which a frame counts as "looking away" (45.0)
        stale_timeout_sec:   If no frame arrives for this long, assume away (4.0)
    """

    def __init__(
        self,
        window_size: int = 5,
        away_pct_threshold: float = 0.50,
        look_pct_threshold: float = 0.50,
        deviation_hold_sec: float = 2.0,
        recovery_entry_sec: float = 0.0,
        recovery_full_sec: float = 2.0,
        gaze_threshold: float = 50.0,
        stale_timeout_sec: float = 5.0,
    ):
        # Configurable thresholds
        self._window_size = window_size
        self._away_pct = away_pct_threshold
        self._look_pct = look_pct_threshold
        self._deviation_hold = deviation_hold_sec
        self._recovery_entry = recovery_entry_sec
        self._recovery_full = recovery_full_sec
        self._gaze_threshold = gaze_threshold
        self._stale_timeout = stale_timeout_sec

        # Rolling window of booleans: True = looking at camera
        self._frame_window: deque = deque(maxlen=window_size)

        # FSM state
        self._state: GazeState = GazeState.ATTENTIVE

        # Timers (epoch seconds, None = not running)
        self._deviation_start: Optional[float] = None   # when away-percentage first exceeded threshold
        self._recovery_start: Optional[float] = None     # when look-percentage first exceeded threshold
        self._last_frame_time: Optional[float] = None    # for staleness detection

    # ── Public API ────────────────────────────────────────

    def update(self, gaze_score: float) -> Dict[str, Any]:
        """
        Feed a new gaze score (0–100) from the detector.
        Returns the current FSM state and metadata.

        Call this once per processed frame (~every 1–2 seconds in this system).
        """
        now = time.time()
        self._last_frame_time = now

        # Classify this frame as looking (True) or away (False)
        is_looking = gaze_score >= self._gaze_threshold
        print(f"[GAZE FSM] score={gaze_score:.1f} threshold={self._gaze_threshold} is_looking={is_looking} state={self._state}")

        # Push into rolling window
        self._frame_window.append(is_looking)

        # Compute window statistics
        total = len(self._frame_window)
        looking_count = sum(self._frame_window)
        away_count = total - looking_count

        looking_pct = looking_count / total if total > 0 else 1.0
        away_pct = away_count / total if total > 0 else 0.0

        # Determine dominant signal from the window
        window_says_away = away_pct >= self._away_pct
        window_says_looking = looking_pct >= self._look_pct

        # ── State transitions ─────────────────────────────
        prev_state = self._state

        if self._state == GazeState.ATTENTIVE:
            self._recovery_start = None  # Not applicable in this state

            if window_says_away:
                # Start or continue deviation timer
                if self._deviation_start is None:
                    self._deviation_start = now

                elapsed = now - self._deviation_start
                if elapsed >= self._deviation_hold:
                    # Sustained deviation → WARNING
                    self._state = GazeState.WARNING_ACTIVE
                    self._deviation_start = None  # Reset — no longer needed
                    self._recovery_start = None
            else:
                # Window is not predominantly away — reset deviation timer
                self._deviation_start = None

        elif self._state == GazeState.WARNING_ACTIVE:
            self._deviation_start = None  # Not applicable in this state

            if is_looking:
                # User looked back → clear warning, enter recovery
                self._state = GazeState.RECOVERING
                self._recovery_start = now
            else:
                # Still looking away — reset any partial recovery
                self._recovery_start = None

        elif self._state == GazeState.RECOVERING:
            self._deviation_start = None

            if is_looking:
                # Still looking at camera — continue recovery timer
                if self._recovery_start is None:
                    self._recovery_start = now

                elapsed = now - self._recovery_start
                if elapsed >= self._recovery_full:
                    # Full recovery achieved → ATTENTIVE
                    self._state = GazeState.ATTENTIVE
                    self._recovery_start = None
            else:
                # FIX: Only fall back to WARNING if the window is predominantly away,
                # not on a single frame — prevents false regression during recovery
                if window_says_away:
                    self._state = GazeState.WARNING_ACTIVE
                    self._recovery_start = None
                # else: single away frame during recovery — ignore, keep recovering

        return self._build_output(gaze_score, looking_pct, away_pct, prev_state)

    def check_staleness(self) -> Dict[str, Any]:
        """
        Call periodically even when no frame arrives.
        If no frame for > stale_timeout, inject an "away" signal.
        Handles camera freeze and frame drops.
        """
        if self._last_frame_time is None:
            return self._build_output(0, 0, 0, self._state)

        elapsed = time.time() - self._last_frame_time
        if elapsed > self._stale_timeout:
            # Inject away frames to fill the gap
            return self.update(0.0)

        return self._build_output(0, 0, 0, self._state)

    def reset(self):
        """Reset FSM to initial state (new session)."""
        self._frame_window.clear()
        self._state = GazeState.ATTENTIVE
        self._deviation_start = None
        self._recovery_start = None
        self._last_frame_time = None

    @property
    def state(self) -> GazeState:
        return self._state

    @property
    def show_warning(self) -> bool:
        """Whether the UI should display a warning overlay."""
        return self._state == GazeState.WARNING_ACTIVE

    # ── Internal ──────────────────────────────────────────

    def _build_output(
        self,
        gaze_score: float,
        looking_pct: float,
        away_pct: float,
        prev_state: GazeState,
    ) -> Dict[str, Any]:
        return {
            "state": self._state.value,
            "show_warning": self._state == GazeState.WARNING_ACTIVE,
            "gaze_score": round(gaze_score, 1),
            "looking_pct": round(looking_pct, 2),
            "away_pct": round(away_pct, 2),
            "state_changed": self._state != prev_state,
            "window_size": len(self._frame_window),
        }


# ══════════════════════════════════════════════════════════════════════


class MultimodalAnalysisEngine:
    """
    Real-time multimodal analysis for interview candidates.
    Processes video frames, audio, and text to produce continuous metrics.
    """

    def __init__(self, window_size: int = 30):
        self.window_size = window_size  # Sliding window for temporal smoothing

        # Temporal buffers for each modality (sliding windows)
        self.emotion_history: deque = deque(maxlen=window_size)
        self.voice_history: deque = deque(maxlen=window_size)
        self.gaze_history: deque = deque(maxlen=window_size)
        self.posture_history: deque = deque(maxlen=window_size)
        self.fluency_history: deque = deque(maxlen=window_size)

        # Cache Haar cascades to avoid reloading on every frame
        self._face_cascade = None
        self._eye_cascade = None
        if CV2_AVAILABLE:
            try:
                self._face_cascade = cv2.CascadeClassifier(
                    cv2.data.haarcascades + "haarcascade_frontalface_default.xml"
                )
                self._eye_cascade = cv2.CascadeClassifier(
                    cv2.data.haarcascades + "haarcascade_eye.xml"
                )
            except Exception:
                pass

        # Fusion weights (learned / configured)
        self.fusion_weights = {
            "emotion": 0.25,
            "voice": 0.20,
            "gaze": 0.15,
            "posture": 0.15,
            "fluency": 0.25,
        }

        # Running metrics
        self._metrics_log: List[Dict[str, Any]] = []
        self._start_time: Optional[float] = None

    def reset(self):
        """Reset all buffers for a new session."""
        self.emotion_history.clear()
        self.voice_history.clear()
        self.gaze_history.clear()
        self.posture_history.clear()
        self.fluency_history.clear()
        self._metrics_log.clear()
        self._start_time = time.time()

    # ── Facial Expression Recognition (FER+) ─────────

    def analyze_face(self, frame_b64: str) -> Dict[str, Any]:
        """Analyze facial expressions from a base64-encoded video frame.

        Uses DeepFace with FER+ backend for emotion recognition.
        Returns emotion scores, confidence, and stability metrics.
        """
        if not CV2_AVAILABLE:
            return self._default_emotion()

        try:
            img_bytes = base64.b64decode(frame_b64)
            nparr = np.frombuffer(img_bytes, np.uint8)
            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

            if frame is None:
                return self._default_emotion()

            return self._process_face(frame)
        except Exception as e:
            return self._default_emotion()

    def _process_face(self, frame: np.ndarray) -> Dict[str, Any]:
        """Process a CV2 frame for facial analysis."""
        result = {
            "dominant_emotion": "neutral",
            "emotion_scores": {},
            "confidence_score": 50.0,
            "emotion_stability": 50.0,
            "face_detected": False,
            "micro_expressions": [],
        }

        if DEEPFACE_AVAILABLE:
            try:
                analysis = DeepFace.analyze(
                    frame, actions=["emotion"],
                    enforce_detection=False, silent=True,
                )
                if isinstance(analysis, list):
                    analysis = analysis[0]

                emotions = analysis.get("emotion", {})
                result["dominant_emotion"] = analysis.get("dominant_emotion", "neutral")
                result["emotion_scores"] = emotions
                result["face_detected"] = True

                # Compute confidence from emotion distribution
                result["confidence_score"] = self._emotion_to_confidence(emotions)

                # Detect micro-expressions (rapid changes)
                result["micro_expressions"] = self._detect_micro_expressions(emotions)

            except Exception:
                pass

        # Gaze estimation from face + eye detection
        result["eye_contact_score"] = self._estimate_gaze(frame)

        # Store in temporal buffer
        self.emotion_history.append({
            "timestamp": time.time(),
            **result,
        })

        # Compute stability from history
        result["emotion_stability"] = self._compute_emotion_stability()

        return result

    def _emotion_to_confidence(self, emotions: Dict[str, float]) -> float:
        """Map emotion distribution to confidence score."""
        happy = emotions.get("happy", 0)
        neutral = emotions.get("neutral", 0)
        surprise = emotions.get("surprise", 0)
        fear = emotions.get("fear", 0)
        sad = emotions.get("sad", 0)
        angry = emotions.get("angry", 0)
        disgust = emotions.get("disgust", 0)

        # Positive indicators
        positive = happy * 0.4 + neutral * 0.35 + surprise * 0.1
        # Negative indicators
        negative = fear * 0.4 + sad * 0.25 + angry * 0.2 + disgust * 0.15

        score = max(0, min(100, 50 + positive - negative))
        return round(score, 1)

    def _detect_micro_expressions(self, current_emotions: Dict[str, float]) -> List[str]:
        """Detect micro-expressions by comparing with recent history."""
        if len(self.emotion_history) < 2:
            return []

        last = self.emotion_history[-1].get("emotion_scores", {})
        micro = []

        for emotion, score in current_emotions.items():
            last_score = last.get(emotion, 0)
            delta = abs(score - last_score)
            if delta > 20:  # Significant rapid change
                direction = "spike" if score > last_score else "drop"
                micro.append(f"{emotion}_{direction}")

        return micro

    def _compute_emotion_stability(self) -> float:
        """Compute emotion stability from temporal history."""
        if len(self.emotion_history) < 3:
            return 50.0

        dominant_emotions = [
            h.get("dominant_emotion", "neutral")
            for h in self.emotion_history
        ]

        # Count transitions
        transitions = sum(
            1 for i in range(1, len(dominant_emotions))
            if dominant_emotions[i] != dominant_emotions[i - 1]
        )
        transition_rate = transitions / max(len(dominant_emotions) - 1, 1)

        # Lower transition rate = more stable
        stability = max(0, min(100, 100 - transition_rate * 100))
        return round(stability, 1)

    def _estimate_gaze(self, frame: np.ndarray) -> float:
        """Estimate eye contact / gaze direction using eye detection.

        Strategy:
        1. Detect face with Haar cascade (more lenient params to reduce false negatives)
        2. Within face ROI, detect eyes with eye cascade
        3. If face found but no eyes detected → looking away → LOW score
        4. If eyes found, check iris position (centering) → gaze score
        5. Also penalise if face itself is off-centre (head turned)
        """
        if not CV2_AVAILABLE or self._face_cascade is None:
            return 50.0

        try:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

            # FIX: More lenient detection params to reduce missed faces
            # scaleFactor 1.1 (was 1.3) — finer pyramid steps
            # minNeighbors 3 (was 5) — less strict confirmation
            # minSize (40,40) (was (60,60)) — catch smaller/farther faces
            faces = self._face_cascade.detectMultiScale(
                gray, scaleFactor=1.1, minNeighbors=3, minSize=(40, 40)
            )

            print(f"[GAZE] frame shape={frame.shape}, faces found={len(faces)}")

            if len(faces) == 0:
                # No face at all — definitely not looking at camera
                raw_score = 10.0
            else:
                (fx, fy, fw, fh) = faces[0]

                # ── Signal 1: Face centering (head turned?) ────────
                frame_cx = frame.shape[1] / 2
                face_cx = fx + fw / 2
                face_offset = abs(face_cx - frame_cx) / frame_cx  # 0 = centered, 1 = edge
                # Aggressive penalty — even a small offset means gaze is shifting
                face_center_score = max(0, 100 - face_offset * 200)

                # ── Signal 2: Eye detection within face ROI ────────
                eye_score = 0.0
                eyes_detected = False

                if self._eye_cascade is not None:
                    # Look for eyes in the UPPER half of the face region
                    eye_roi_gray = gray[fy:fy + int(fh * 0.65), fx:fx + fw]

                    # FIX: More lenient eye detection params
                    # scaleFactor 1.05 (was 1.1) — finer steps for small eyes
                    # minNeighbors 3 (was 4) — less strict
                    # minSize (15,15) (was (20,20)) — catch smaller eyes
                    eyes = self._eye_cascade.detectMultiScale(
                        eye_roi_gray,
                        scaleFactor=1.05,
                        minNeighbors=3,
                        minSize=(15, 15),
                    )

                    if len(eyes) >= 2:
                        # Both eyes visible → likely looking at camera
                        eyes_detected = True

                        # Check eye symmetry — if both eyes are roughly
                        # at similar Y and symmetrically placed in the face,
                        # person is looking forward
                        eyes_sorted = sorted(eyes, key=lambda e: e[0])  # sort by x
                        e1 = eyes_sorted[0]
                        e2 = eyes_sorted[1]

                        # Horizontal symmetry: both eyes equally spaced from face centre
                        e1_cx = (e1[0] + e1[2] / 2) / fw  # normalised 0..1
                        e2_cx = (e2[0] + e2[2] / 2) / fw
                        mid = (e1_cx + e2_cx) / 2
                        symmetry_offset = abs(mid - 0.5)  # 0 = perfectly centred

                        # Vertical alignment: both eyes at similar height
                        e1_cy = e1[1] + e1[3] / 2
                        e2_cy = e2[1] + e2[3] / 2
                        y_diff = abs(e1_cy - e2_cy) / fh

                        if symmetry_offset < 0.10 and y_diff < 0.08:
                            eye_score = 90.0  # looking straight at camera
                        elif symmetry_offset < 0.15:
                            eye_score = 55.0  # slightly off-centre gaze
                        else:
                            eye_score = 20.0  # eyes asymmetric → looking aside

                    elif len(eyes) == 1:
                        # Only one eye visible → partially turned away
                        eyes_detected = True
                        eye_score = 15.0
                    else:
                        # No eyes detected in face ROI → looking away or eyes closed
                        eye_score = 10.0

                # ── Combine signals ────────────────────────────────
                if eyes_detected:
                    # Weight: 50% eye analysis, 50% face centering
                    raw_score = eye_score * 0.5 + face_center_score * 0.5
                else:
                    # Face found but no eyes — heavily penalise
                    raw_score = min(face_center_score * 0.2, 20.0)

            # ── Temporal smoothing (lighter — 70/30 to keep responsiveness) ──
            recent_scores = [
                g["score"] for g in self.gaze_history
                if time.time() - g["timestamp"] < 3  # last 3 seconds
            ]
            if recent_scores:
                avg_recent = sum(recent_scores) / len(recent_scores)
                gaze_score = raw_score * 0.7 + avg_recent * 0.3
            else:
                gaze_score = raw_score

            self.gaze_history.append({
                "timestamp": time.time(),
                "score": gaze_score,
                "face_detected": len(faces) > 0,
            })
            return round(gaze_score, 1)

        except Exception:
            return 50.0

    def detect_persons(self, frame_b64: str) -> int:
        """Count the number of persons visible using YOLOv8.

        Returns the person count (class 0 = 'person' in COCO).
        If YOLO is unavailable, falls back to Haar face count.
        """
        if not CV2_AVAILABLE:
            return 0

        try:
            img_bytes = base64.b64decode(frame_b64)
            nparr = np.frombuffer(img_bytes, np.uint8)
            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            if frame is None:
                return 0

            if YOLO_AVAILABLE and _yolo_model is not None:
                results = _yolo_model(frame, verbose=False)
                person_count = 0
                for r in results:
                    for box in r.boxes:
                        if int(box.cls[0]) == 0 and float(box.conf[0]) >= 0.45:
                            person_count += 1
                return person_count

            # Fallback: Haar cascade face count
            if self._face_cascade is not None:
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                faces = self._face_cascade.detectMultiScale(
                    gray, scaleFactor=1.1, minNeighbors=4, minSize=(50, 50)
                )
                return len(faces)

            return 0
        except Exception:
            return 0

    def _default_emotion(self) -> Dict[str, Any]:
        return {
            "dominant_emotion": "neutral",
            "emotion_scores": {},
            "confidence_score": 50.0,
            "emotion_stability": 50.0,
            "face_detected": False,
            "micro_expressions": [],
            "eye_contact_score": 50.0,
        }

    # ── Voice Sentiment Analysis ──────────────────────

    def analyze_voice(
        self,
        audio_features: Optional[Dict[str, float]] = None,
        transcript: str = "",
    ) -> Dict[str, Any]:
        """Analyze voice characteristics for sentiment and confidence.

        Features extracted (externally or from Wav2Vec2):
          - pitch_mean, pitch_std: Voice pitch statistics
          - energy: Voice volume/energy level
          - speaking_rate: Words per minute
          - pause_ratio: Ratio of silence to speech
          - jitter: Pitch variation (nervousness indicator)
          - shimmer: Amplitude variation
        """
        if audio_features is None:
            audio_features = self._default_voice_features()

        # Extract sentiment indicators from voice features
        pitch_mean = audio_features.get("pitch_mean", 150)
        pitch_std = audio_features.get("pitch_std", 30)
        energy = audio_features.get("energy", 0.5)
        speaking_rate = audio_features.get("speaking_rate", 120)
        pause_ratio = audio_features.get("pause_ratio", 0.3)
        jitter = audio_features.get("jitter", 0.02)

        # Confidence from voice
        voice_confidence = 50.0
        if energy > 0.6 and speaking_rate > 100:
            voice_confidence += 20
        if jitter < 0.03:  # Low jitter = steady voice
            voice_confidence += 15
        if pause_ratio < 0.4:
            voice_confidence += 10
        voice_confidence = min(100, max(0, voice_confidence))

        # Stress from voice
        stress_level = 50.0
        if pitch_std > 40:  # High pitch variation
            stress_level += 20
        if jitter > 0.04:
            stress_level += 15
        if pause_ratio > 0.5:
            stress_level += 10
        stress_level = min(100, max(0, stress_level))

        # Engagement
        engagement = 50.0
        if speaking_rate > 110 and energy > 0.5:
            engagement += 25
        if pitch_std > 20:  # Some natural variation
            engagement += 10
        engagement = min(100, max(0, engagement))

        result = {
            "voice_confidence": round(voice_confidence, 1),
            "stress_level": round(stress_level, 1),
            "engagement": round(engagement, 1),
            "speaking_rate_wpm": round(speaking_rate, 1),
            "pause_ratio": round(pause_ratio, 2),
            "pitch_stability": round(max(0, 100 - pitch_std), 1),
            "energy_level": round(energy * 100, 1),
        }

        self.voice_history.append({
            "timestamp": time.time(),
            **result,
        })

        return result

    def _default_voice_features(self) -> Dict[str, float]:
        return {
            "pitch_mean": 150,
            "pitch_std": 30,
            "energy": 0.5,
            "speaking_rate": 120,
            "pause_ratio": 0.3,
            "jitter": 0.02,
            "shimmer": 0.03,
        }

    # ── Speech Fluency Metrics ────────────────────────

    def analyze_fluency(self, transcript: str, duration_seconds: float) -> Dict[str, Any]:
        """Analyze speech fluency from transcript."""
        if not transcript.strip():
            return {
                "fluency_score": 0,
                "words_per_minute": 0,
                "filler_word_count": 0,
                "filler_ratio": 0,
                "sentence_completeness": 0,
                "vocabulary_richness": 0,
                "clarity_score": 0,
            }

        words = transcript.split()
        word_count = len(words)
        wpm = (word_count / max(duration_seconds, 1)) * 60

        # Filler word detection
        filler_words = {
            "um", "uh", "like", "you know", "basically", "actually",
            "literally", "sort of", "kind of", "i mean", "right",
            "so", "well", "okay", "hmm",
        }
        transcript_lower = transcript.lower()
        filler_count = sum(
            transcript_lower.count(f) for f in filler_words
        )
        filler_ratio = filler_count / max(word_count, 1)

        # Sentence completeness
        sentences = [s.strip() for s in transcript.split(".") if s.strip()]
        avg_sentence_length = sum(len(s.split()) for s in sentences) / max(len(sentences), 1)
        completeness = min(100, avg_sentence_length * 8)

        # Vocabulary richness (type-token ratio)
        unique_words = len(set(w.lower() for w in words))
        vocabulary_richness = (unique_words / max(word_count, 1)) * 100

        # Overall fluency score
        fluency = 50.0
        if 100 <= wpm <= 160:
            fluency += 20  # Optimal speaking rate
        elif 80 <= wpm <= 180:
            fluency += 10
        if filler_ratio < 0.05:
            fluency += 15
        elif filler_ratio < 0.10:
            fluency += 5
        if vocabulary_richness > 60:
            fluency += 10
        if completeness > 50:
            fluency += 5
        fluency = min(100, max(0, fluency))

        # Clarity score
        clarity = min(100, fluency * 0.4 + (100 - filler_ratio * 200) * 0.3 + completeness * 0.3)

        result = {
            "fluency_score": round(fluency, 1),
            "words_per_minute": round(wpm, 1),
            "filler_word_count": filler_count,
            "filler_ratio": round(filler_ratio, 3),
            "sentence_completeness": round(completeness, 1),
            "vocabulary_richness": round(vocabulary_richness, 1),
            "clarity_score": round(max(0, clarity), 1),
            "word_count": word_count,
        }

        self.fluency_history.append({
            "timestamp": time.time(),
            **result,
        })

        return result

    # ── Attention-Based Cross-Modal Fusion ────────────

    def compute_fused_metrics(self) -> Dict[str, Any]:
        """
        Attention-based fusion of all modalities into unified metrics.

        Fusion Mechanism:
          For each metric, compute attention-weighted average across modalities.
          Attention weights are based on:
            1. Static importance weights (self.fusion_weights)
            2. Signal quality / availability
            3. Temporal consistency (more stable signals get higher weight)
        """
        # Get latest readings from each modality
        emotion = self.emotion_history[-1] if self.emotion_history else {}
        voice = self.voice_history[-1] if self.voice_history else {}
        fluency = self.fluency_history[-1] if self.fluency_history else {}
        gaze = self.gaze_history[-1] if self.gaze_history else {}

        # Compute dynamic attention weights
        weights = self._compute_attention_weights()

        # ── Fused Confidence Score ────────────────────
        confidence_sources = []
        confidence_weights = []

        if emotion.get("confidence_score") is not None:
            confidence_sources.append(emotion["confidence_score"])
            confidence_weights.append(weights.get("emotion", 0.25))

        if voice.get("voice_confidence") is not None:
            confidence_sources.append(voice["voice_confidence"])
            confidence_weights.append(weights.get("voice", 0.20))

        if fluency.get("fluency_score") is not None:
            confidence_sources.append(fluency["fluency_score"])
            confidence_weights.append(weights.get("fluency", 0.25))

        fused_confidence = self._weighted_average(
            confidence_sources, confidence_weights
        )

        # ── Fused Stress Level ────────────────────────
        stress_sources = []
        stress_weights = []

        # From emotion: inverse of stability
        if emotion.get("emotion_stability") is not None:
            stress_sources.append(100 - emotion["emotion_stability"])
            stress_weights.append(weights.get("emotion", 0.25))

        if voice.get("stress_level") is not None:
            stress_sources.append(voice["stress_level"])
            stress_weights.append(weights.get("voice", 0.30))

        fused_stress = self._weighted_average(stress_sources, stress_weights)

        # ── Fused Attention Index ─────────────────────
        attention_sources = []
        attention_weights = []

        if gaze.get("score") is not None:
            attention_sources.append(gaze["score"])
            attention_weights.append(0.4)

        if emotion.get("face_detected"):
            attention_sources.append(80.0)
            attention_weights.append(0.3)

        if voice.get("engagement") is not None:
            attention_sources.append(voice["engagement"])
            attention_weights.append(0.3)

        fused_attention = self._weighted_average(
            attention_sources, attention_weights
        )

        # ── Fused Emotional Stability ─────────────────
        stability = emotion.get("emotion_stability", 50.0)

        # ── Speech Clarity ────────────────────────────
        clarity = fluency.get("clarity_score", 50.0)

        # ── Answer Completeness (from fluency) ────────
        completeness = fluency.get("sentence_completeness", 50.0)

        # ── Compute overall performance score ─────────
        overall = (
            fused_confidence * 0.25 +
            (100 - fused_stress) * 0.15 +
            fused_attention * 0.15 +
            stability * 0.15 +
            clarity * 0.15 +
            completeness * 0.15
        )

        metrics = {
            "timestamp": datetime.utcnow().isoformat(),
            "confidence_score": round(fused_confidence, 1),
            "stress_level": round(fused_stress, 1),
            "attention_index": round(fused_attention, 1),
            "emotional_stability": round(stability, 1),
            "speech_clarity": round(clarity, 1),
            "answer_completeness": round(completeness, 1),
            "overall_performance": round(overall, 1),
            # Detailed per-modality scores
            "modality_scores": {
                "emotion": {
                    "dominant_emotion": emotion.get("dominant_emotion", "neutral"),
                    "confidence": emotion.get("confidence_score", 50),
                    "stability": stability,
                },
                "voice": {
                    "confidence": voice.get("voice_confidence", 50),
                    "stress": voice.get("stress_level", 50),
                    "engagement": voice.get("engagement", 50),
                },
                "gaze": {
                    "eye_contact": gaze.get("score", 50),
                    "face_detected": gaze.get("face_detected", False),
                },
                "fluency": {
                    "score": fluency.get("fluency_score", 50),
                    "clarity": clarity,
                    "wpm": fluency.get("words_per_minute", 0),
                    "filler_ratio": fluency.get("filler_ratio", 0),
                },
            },
            "fusion_weights": weights,
        }

        self._metrics_log.append(metrics)
        return metrics

    def _compute_attention_weights(self) -> Dict[str, float]:
        """Compute dynamic attention weights based on signal availability and quality."""
        weights = dict(self.fusion_weights)

        # Reduce weight for modalities with no data
        if not self.emotion_history:
            weights["emotion"] = 0.0
        if not self.voice_history:
            weights["voice"] = 0.0
        if not self.gaze_history:
            weights["gaze"] = 0.0
        if not self.fluency_history:
            weights["fluency"] = 0.0

        # Normalize weights
        total = sum(weights.values())
        if total > 0:
            weights = {k: v / total for k, v in weights.items()}

        return weights

    def _weighted_average(self, values: List[float], weights: List[float]) -> float:
        """Compute weighted average."""
        if not values:
            return 50.0
        total_weight = sum(weights)
        if total_weight == 0:
            return sum(values) / len(values)
        return sum(v * w for v, w in zip(values, weights)) / total_weight

    # ── Temporal Trend Analysis ───────────────────────

    def get_temporal_trends(self) -> Dict[str, Any]:
        """Analyze trends across the interview session."""
        if len(self._metrics_log) < 3:
            return {"trend": "insufficient_data", "data_points": len(self._metrics_log)}

        # Extract time series for key metrics
        confidence_series = [m["confidence_score"] for m in self._metrics_log]
        stress_series = [m["stress_level"] for m in self._metrics_log]
        attention_series = [m["attention_index"] for m in self._metrics_log]

        def compute_trend(series: List[float]) -> str:
            if len(series) < 3:
                return "stable"
            first_half = np.mean(series[:len(series) // 2])
            second_half = np.mean(series[len(series) // 2:])
            diff = second_half - first_half
            if diff > 5:
                return "improving"
            elif diff < -5:
                return "declining"
            return "stable"

        return {
            "confidence_trend": compute_trend(confidence_series),
            "stress_trend": compute_trend(stress_series),
            "attention_trend": compute_trend(attention_series),
            "confidence_avg": round(float(np.mean(confidence_series)), 1),
            "stress_avg": round(float(np.mean(stress_series)), 1),
            "attention_avg": round(float(np.mean(attention_series)), 1),
            "data_points": len(self._metrics_log),
            "session_duration_seconds": (
                time.time() - self._start_time if self._start_time else 0
            ),
        }

    # ── Session Summary ───────────────────────────────

    def get_session_summary(self) -> Dict[str, Any]:
        """Get comprehensive session analysis summary."""
        if not self._metrics_log:
            return {"status": "no_data"}

        all_confidence = [m["confidence_score"] for m in self._metrics_log]
        all_stress = [m["stress_level"] for m in self._metrics_log]
        all_attention = [m["attention_index"] for m in self._metrics_log]
        all_stability = [m["emotional_stability"] for m in self._metrics_log]
        all_clarity = [m["speech_clarity"] for m in self._metrics_log]
        all_overall = [m["overall_performance"] for m in self._metrics_log]

        return {
            "total_data_points": len(self._metrics_log),
            "averages": {
                "confidence": round(float(np.mean(all_confidence)), 1),
                "stress": round(float(np.mean(all_stress)), 1),
                "attention": round(float(np.mean(all_attention)), 1),
                "stability": round(float(np.mean(all_stability)), 1),
                "clarity": round(float(np.mean(all_clarity)), 1),
                "overall": round(float(np.mean(all_overall)), 1),
            },
            "peaks": {
                "max_confidence": round(float(np.max(all_confidence)), 1),
                "max_stress": round(float(np.max(all_stress)), 1),
                "min_attention": round(float(np.min(all_attention)), 1),
            },
            "trends": self.get_temporal_trends(),
            "recommendations": self._generate_behavioral_recommendations(),
        }

    def _generate_behavioral_recommendations(self) -> List[str]:
        """Generate actionable recommendations based on multimodal analysis."""
        recommendations = []

        if not self._metrics_log:
            return ["Complete a practice session to receive personalized recommendations."]

        avg_confidence = np.mean([m["confidence_score"] for m in self._metrics_log])
        avg_stress = np.mean([m["stress_level"] for m in self._metrics_log])
        avg_attention = np.mean([m["attention_index"] for m in self._metrics_log])
        avg_clarity = np.mean([m["speech_clarity"] for m in self._metrics_log])

        if avg_confidence < 50:
            recommendations.append(
                "Practice power posing before interviews — research shows it boosts felt confidence by 20%."
            )
        if avg_stress > 60:
            recommendations.append(
                "Try box breathing (4-4-4-4) between questions to reduce stress indicators."
            )
        if avg_attention < 50:
            recommendations.append(
                "Maintain steady eye contact with the camera. Place a sticky note near it as a reminder."
            )
        if avg_clarity < 50:
            recommendations.append(
                "Slow your speaking rate and reduce filler words. Practice with a timer for structured responses."
            )

        if not recommendations:
            recommendations.append(
                "Strong performance across all metrics. Continue practicing to maintain consistency."
            )

        return recommendations


# Singleton
multimodal_engine = MultimodalAnalysisEngine()